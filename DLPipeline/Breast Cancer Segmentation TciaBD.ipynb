{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a072ec3",
   "metadata": {},
   "source": [
    "# Breast Cancer Segmentation TciaBD\n",
    "\n",
    "We will designing and developing MSGRAP DL model based on H. Lee's research paper. We will adapt it for be trained on breast cancer dataset (CT, MRI, PET) since H. Lee originally trained it on breast cancer ultrasound images. We will note down the differences as we work out the implementation.\n",
    "\n",
    "[1] H. Lee, J. Park and J. Y. Hwang, \"Channel Attention Module With Multiscale Grid Average Pooling for Breast Cancer Segmentation in an Ultrasound Image,\" in IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control, vol. 67, no. 7, pp. 1344-1353, July 2020, doi: [10.1109/TUFFC.2020.2972573](https://ieeexplore-ieee-org.libaccess.sjlibrary.org/document/8988165)\n",
    "\n",
    "Referenced 2D Channel - Spatial Attention (Squeeze & Excite) VGG based TensorFlow Keras code in tutorial [Attending to Channels Using Keras and TensorFlow](https://pyimagesearch.com/2022/05/30/attending-to-channels-using-keras-and-tensorflow/)\n",
    "\n",
    "## Outline\n",
    "\n",
    "- Prepare Tcia Breast Diagnosis Data\n",
    "- Breast Segmentation Model Architecture\n",
    "- Train Breast Segmentation ML/DL Models\n",
    "- Evaluate Breast Segmentation ML/DL Models Quantitatively\n",
    "- Evaluate Breast Segmentation ML/DL Models Qualitatively\n",
    "- Deploy Breast Segmentation DL Model for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ada06244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-19 20:46:35.927064: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-19 20:46:36.487513: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-11-19 20:46:36.487563: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-11-19 20:46:36.566579: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-11-19 20:46:37.773439: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-11-19 20:46:37.773503: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.7/lib64\n",
      "2022-11-19 20:46:37.773509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Dropout\n",
    "from tensorflow.keras.layers import Multiply, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization, Flatten, Conv2D, AveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b18d56",
   "metadata": {},
   "source": [
    "## Prepare Tcia Breast Diagnosis Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e70a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d19df8a",
   "metadata": {},
   "source": [
    "## Breast Segmentation Model Architecture\n",
    "\n",
    "DL MSGRAP Breast Cancer Segmentation Architecture:\n",
    "\n",
    "- Encoder\n",
    "    - Based on VGGNet except for batch normalization and channel attention modules\n",
    "    - Use all conv layers with C' filters with size 3x3xC\n",
    "    - Upsample the feature maps using a 4x4 transpose conv with a stride of 2\n",
    "    - Unlike UNet, only 2 feature maps from encoder are connected to decoder\n",
    "- Decoder\n",
    "    - symmetrical encoder architecture built into decoder\n",
    "    - Use all conv layers with C' filters with size 3x3xC\n",
    "    - Final conv layer has 2 filters with size 3x3x64\n",
    "    - Obtain final segmentation results into binary classes via argmax with threshold 0.5\n",
    "\n",
    "The network receives a breast ultrasound image as input and predicts its semantic segmentation result.\n",
    "\n",
    "- Note: C and C' are the previous and current number of feature maps, except for the final conv layer\n",
    "\n",
    "- Note: batch normalization is highly influenced by a batch size: the smaller the batch size, the lower the performance is. Small batch size reduces the generalization ability. H. Lee et al uses group batch normalization since it has little effect on batch size and the dataset used in experiments had enough spatial size\n",
    "\n",
    "- Note: group normalization divides each channel into N groups and normalizes the features within each group regardless of the batch size. Thus, it doesnt depend on the batch size and can overcome the generalization issues caused by the small batch size when the network is trained with large input imagess.\n",
    "\n",
    "- Note: After performing additional experiment, H. Lee et al demonstrated that their  network architecture with 2 feature maps connected between the encoder and decoder performed better than UNet like architectures.\n",
    "    - It is better to not use low-level features in the network since most ultrasound images are noisy. (similar for PET, SPECT).\n",
    "    \n",
    "- Note: H. Lee et al also conducts the ablation study for architecture with two of the tenfold datasets.\n",
    "\n",
    "- Note: Semantic segmentation F1 Score: UNet = 0.78, H. Lee's MSGRAP = 0.79\n",
    "\n",
    "![msgrap_h_lee_breast_cancer_segmentation](./msgrap_h_lee_breast_cancer_segmentation.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c63d29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model():\n",
    "    def __init__(self, input_shape, ratio, blocks, num_classes,\n",
    "        dense_units, conv_filters, is_squeeze_excite, optimizer,\n",
    "        loss, metrics):\n",
    "        self.input_shape = input_shape\n",
    "        self.rate = ratio\n",
    "        self.block = blocks\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.dense_units = dense_units\n",
    "        self.conv_filters = conv_filters\n",
    "        \n",
    "        # Flag decides whether to add squeeze excitation layer\n",
    "        # to network. This is channel - spatial attention layer\n",
    "        self.is_squeeze_excite = is_squeeze_excite\n",
    "        \n",
    "        self.optimizer = optimizer\n",
    "        self.loss = loss\n",
    "        self.metrics = metrics\n",
    "        pass\n",
    "\n",
    "    def split(self, resized_img, resized_mask):\n",
    "        pass\n",
    "    \n",
    "    def data_gen(self, img_list, mask_list, batch_size):\n",
    "        pass\n",
    "    \n",
    "#     def conv_stem(self, inputs, filters=3, kernel_size=3):\n",
    "#         # pass the input through a CONV => ReLU layer block\n",
    "#         x = Conv3D(filters = filters, kernel_size = (kernel_size, kernel_size, kernel_size),\n",
    "#             kernel_initializer = \"he_normal\", padding = \"same\")(inputs)\n",
    "#         x = Activation(\"relu\")(x)\n",
    "#         x = GroupNormalization()(x)\n",
    "#         if self.is_squeeze_excite:\n",
    "#             x = self.squeeze_excite_block(x)\n",
    "#             x = Activation('relu')(x)\n",
    "#         return x\n",
    "    \n",
    "#     def learner(self, x):\n",
    "#         # build the learner by stacking 3D convolutional layer blocks\n",
    "#         for num_layers, num_filters in self.blocks:\n",
    "#             x = self.convolutional_block(x, num_layers, num_filters)\n",
    "#         return x\n",
    "    \n",
    "    def convolutional_block(self, x, filters=3, kernel_size=3, num_blocks=2):\n",
    "        \"\"\"conv layer followed by group normalization\"\"\"\n",
    "        \"\"\" iterate over the number of layers and build a block\n",
    "            with 3D convolutional layers\"\"\"\n",
    "        for i in range(num_blocks):\n",
    "            x = Conv3D(filters = filters, kernel_size = (kernel_size, kernel_size, kernel_size),\n",
    "                       kernel_initializer = 'he_normal', padding = 'same')(x)\n",
    "            x = Activation('relu')(x)\n",
    "            x = GroupNormalization()(x)\n",
    "            if self.is_squeeze_excite:\n",
    "                x = self.squeeze_excite_block(x)\n",
    "                x = Activation('relu')(x)\n",
    "        return x\n",
    "    \n",
    "    def squeeze_excite_block(self, x):\n",
    "        # store the input\n",
    "        shortcut = x\n",
    "        \n",
    "        # calculate the number of filters the input has\n",
    "        filters = x.shape[-1]\n",
    "        \n",
    "        # the squeeze operation reduces the input dimensionality\n",
    "        # here we do a global average pooling across the filters,\n",
    "        # which reduces the input to a 2D feature map\n",
    "        x = GlobalAveragePooling3D(keepdims=True)(x)\n",
    "    \n",
    "        # reduce the number of filters to C/r\n",
    "        x = Dense(filters // self.ratio, activation=\"relu\",\n",
    "            kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "    \n",
    "        # the excitation operation restores the input dimensionality\n",
    "        x = Dense(filters, activation=\"sigmoid\",\n",
    "            kernel_initializer=\"he_normal\", use_bias=False)(x)\n",
    "        \n",
    "        # multiply the attention weights with the original input\n",
    "        x = Multiply()([shortcut, x])\n",
    "    \n",
    "        # return the output of the SE block\n",
    "        return x\n",
    "    \n",
    "    def msgrap(input_img, filters=64, dropout=0.2):\n",
    "        \"\"\"MSGRAP based on 3D UNet and Squeeze Excitation Network\n",
    "           Squeeze Excitation is the channel - spatial attention layer(s)\n",
    "           This implementation is based on the diagram above. It initially\n",
    "           is initially designed for breast cancer ultrasound, so we will\n",
    "           see how it does for CT, MRI, PET.\n",
    "        \"\"\"\n",
    "        # 64 filters for first two 3D conv blocks\n",
    "        conv1 = convolutional_block(input_img, filters*1, kernel_size=3,\n",
    "                    num_blocks=2)\n",
    "        \n",
    "        # MaxPooling3D, then 128 filters for next two 3D conv blocks\n",
    "        pool2 = MaxPooling3D((2,2,2))(conv1)\n",
    "        conv2 = convolutional_block(pool2, filters*2, kernel_size=3,\n",
    "                    num_blocks=2)\n",
    "        \n",
    "        # MaxPooling3D, then 256 filters for next three 3D conv blocks\n",
    "        pool3 = MaxPooling3D((2,2,2))(conv2)\n",
    "        conv3 = convolutional_block(pool3, filters*4, kernel_size=3,\n",
    "                    num_blocks=3)\n",
    "        \n",
    "        # MaxPooling3D, then 512 filters for next three 3D conv blocks\n",
    "        pool4 = MaxPooling3D((2,2,2))(conv3)\n",
    "        conv4 = convolutional_block(pool4, filters*8, kernel_size=3,\n",
    "                    num_blocks=3)\n",
    "        \n",
    "        # MaxPooling3D, then 512 filters for next three 3D conv blocks\n",
    "        pool5 = MaxPooling3D((2,2,2))(conv4)\n",
    "        conv5 = convolutional_block(pool5, filters*8, kernel_size=3,\n",
    "                    num_blocks=3)\n",
    "        \n",
    "        # Conv3DTranspose, concatenation w/ conv4, then 512 filters for\n",
    "        # next three 3D conv blocks\n",
    "        ups6 = Conv3DTranspose(filters*8, (4,4,4), strides=(2,2,2), padding=\"same\",\n",
    "                              activation=\"relu\", kernel_initializer=\"he_normal\")(conv5)\n",
    "        ups6 = concatenate([ups6, conv4])\n",
    "        conv6 = convolutional_block(ups6, filters*8, kernel_size=3,\n",
    "                    num_blocks=3)\n",
    "        \n",
    "        # Conv3DTranspose, concatenation w/ conv3, then 256 filters for\n",
    "        # next three 3D conv blocks\n",
    "        ups7 = Conv3DTranspose(filters*4, (4,4,4), strides=(2,2,2), padding=\"same\",\n",
    "                              activation=\"relu\", kernel_initializer=\"he_normal\")(conv6)\n",
    "        ups7 = concatentate([ups7, conv3])\n",
    "        conv7 = convolutional_block(ups7, filters*4, kernel_size=3,\n",
    "                    num_blocks=3)\n",
    "        \n",
    "        # Conv3DTranpose, then 128 filters for next two 3D conv blocks\n",
    "        ups8 = Conv3DTranpose(filters*2, (4,4,4), strides=(2,2,2), padding=\"same\",\n",
    "                             activation=\"relu\", kernel_initializer=\"he_normal\")(conv7)\n",
    "        conv8 = convolutional_block(ups8, filters*2, kernel_size=3,\n",
    "                    num_blocks=2)\n",
    "        \n",
    "        # Conv3DTranspose, then 64 filters for next two 3D conv blocks\n",
    "        ups9 = Conv3DTranspose(filters*1, (4,4,4), strides=(2,2,2), padding=\"same\",\n",
    "                              activation=\"relu\", kernel_initializer=\"he_normal\")(conv8)\n",
    "        conv9 = convolutional_block(ups9, filters*1, kernel_size=3,\n",
    "                    num_blocks=2)\n",
    "        # Final conv layer has 2 filters with size 3x3x64\n",
    "        # Obtain final segmentation results into binary classes via argmax with threshold 0.5\n",
    "        outputs = Conv3D(2, (3,3,64), activation=\"sigmoid\", padding=\"same\")(conv9)\n",
    "        model = Model(inputs=[input_img], outputs=[outputs])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fa48ec",
   "metadata": {},
   "source": [
    "## Train Breast Segmentation ML/DL Models\n",
    "\n",
    "To evaluate the performance of their proposed networks (MSGRAP, etc), H. Lee et al trained different models, such as \n",
    "\n",
    "- FCN\n",
    "- UNet\n",
    "- SegNet\n",
    "- PSPNet-18\n",
    "\n",
    "Then compared their performance.\n",
    "\n",
    "Loss function:\n",
    "\n",
    "`L_theta_k_D = -(M-1)_sum_(c=0) (GT_c)log(f(I_theta)_c)`\n",
    "\n",
    "- Note: GT_c is the predicted probability and the binary indicator for the class, c\n",
    "- Note: breast cancer segmentation is a binary classification for each pixel, use `M = 2`\n",
    "\n",
    "- Note: since the breast ultrasound cancer datasets were limited, H. Lee et al  configured the training and testing processes as tenfold cross-validation.\n",
    "\n",
    "- Note: Divided the data into 146 or 147 breast cancer ultrasound images for training\n",
    "    - 16 or 17 breast cancer ultrasound images for testing in each validation step\n",
    "\n",
    "- Note: H. Lee et al agumented each patch by random horizontal and vertical flips and random 90 deg rotations\n",
    "\n",
    "- Note: for training and testing, all image sizes were set to average 454x537 pixels\n",
    "\n",
    "- Note: for training, the weights of all conv layers were initialized `Kaiming initialization`\n",
    "\n",
    "Note: Adam optimization method with parameters was used:\n",
    "\n",
    "- Beta_1 = 0.9\n",
    "- Beta_2 = 0.999\n",
    "- epsilon = 10^-8\n",
    "\n",
    "- Note: `Learning rate = 10^-3` and was `reduced by half` every `30 epochs`\n",
    "\n",
    "- Note: `mini-batch size = 8`\n",
    "\n",
    "- Note: models were trained for `120 epochs`\n",
    "\n",
    "- Note: PyTorch was used to implement and train networks\n",
    "\n",
    "NOTE: it took four days to train the models using:\n",
    "\n",
    "- Intel Zeon E5-2620 at 2.0 GHz\n",
    "- NVIDIA TITAN RTX (24GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc23f76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd66f841",
   "metadata": {},
   "source": [
    "## Evaluate Breast Segmentation ML/DL Models Quantitatively\n",
    "\n",
    "### Performance Metrics\n",
    "\n",
    "For quantitative comparisions of DL MSGRAP with other methods, H. Lee et al used global accuracy, F1 score, sensitivity, specificity.\n",
    "\n",
    "Accuracy = `(TP+TN)/(TP+FP+FN+TN)`\n",
    "\n",
    "F1 = `(2*TP)/(2*TP+FP+FN)`\n",
    "\n",
    "IoU = `TP/(TP+FP+FN)`\n",
    "\n",
    "- Note: Accuracy is most basic metric for several CV tasks\n",
    "- Note: F1 score is good for imbalanced data. For ex, in H. Lee et al's dataset, there was a small portion of cancer among all breast ultrasound images, this data set can be considered imbalanced.\n",
    "- Note: this dataset was imbalanced, it consists of 5% of cancer pixels and 95% of nomal pixels. So, H. Lee et al used FPR, precision, intersection over union (IoU), and area under the curve (AUC) of precision and recall (PR) for fair evaluation.\n",
    "- Note: AUC metrics used a sweep of the threshold from `p=0` to `p=1` as opposed to `p=0.5` (argmax) used for the remaining non-AUC metrics.\n",
    "- Note: FPR is number of false positive over the one of the condition negatives.\n",
    "- Note: IoU metric is commonly used in semantic segmentation\n",
    "\n",
    "Show metrics in a table for models:\n",
    "\n",
    "- FCN, UNet, SegNet, PSPNet-18, ENCNet-18\n",
    "- Ours-GAP, Ours-GRAP, Ours-MSGRAP\n",
    "\n",
    "H. Lee et al didn't use semantic segmentation networks based on ResNet-34, 51 and DesneNet that have more than 18 conv layers, so there would be fair comparisons since those networks have many more parameters than theirs.\n",
    "\n",
    "- Ours-MSGRAP had better `F1 score = 0.7658` than other models\n",
    "    - FCN = 0.7123, UNet = 0.7132, SegNet = 0.7225, PSPNet-18 = 0.7520, ENCNet-18 = 0.7266\n",
    "    \n",
    "Ours-MSGRAP showed higher performance than other models in global accuracy, specificity, FPR, precision and IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7613771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
